# Solar Performance Analytics Platform

Cloud-based pipeline for ISO 61724-compliant solar monitoring analysis.

## Project Overview

Migration of a paid university research collaboration project ("Beca de colaboraciÃ³n" at UPM) â€” 10 years of solar performance data â€” from CSV to a modern cloud data stack.

## Author

Sergio â€” Electrical Engineer transitioning to Data Engineering
Target: Data Engineer in Zurich (Energy Sector)

## Data

| Type | Count | Date Range |
|------|-------|------------|
| Monthly CSVs | 131 | Feb 2013 â†’ Dec 2023 |
| Auxiliary CSVs | 2 | â€” |
| Auxiliary XLSX | 13 | â€” |

## Architecture

| Layer | Technology | Status |
|-------|------------|--------|
| Storage | AWS S3 | âœ… |
| Infrastructure | Terraform | âœ… |
| Orchestration | Airflow | ðŸ”œ |
| Transform | Spark + dbt | ðŸ”œ |
| Visualization | Streamlit | ðŸ”œ |

## S3 Structure

solar-analytics-raw-scl-dev/
â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ monthly/ # 131 monthly CSV files
â”‚ â”‚ â””â”€â”€ year=YYYY/
â”‚ â”‚ â””â”€â”€ month=MM/
â”‚ â””â”€â”€ auxiliary/
â”‚ â”œâ”€â”€ csv/ # 2 auxiliary CSVs
â”‚ â””â”€â”€ xlsx/ # 13 auxiliary XLSX
â”œâ”€â”€ staging/
â”‚ â””â”€â”€ monthly/ # Cleaned Parquet files
â””â”€â”€ curated/
â””â”€â”€ performance_ratio/ # Final analytics


## Infrastructure

### Resources Created

| Resource | Name |
|----------|------|
| S3 Bucket | `solar-analytics-raw-scl-dev` |
| IAM Role | `solar-analytics-pipeline-role-dev` |
| IAM Policy | `solar-analytics-s3-access-dev` |

### Security

- âœ… S3 versioning enabled
- âœ… S3 public access blocked
- âœ… IAM role with least-privilege S3 access

### Deploy

```bash
cd terraform
terraform init
terraform apply

## Data Pipeline

### Source Data
- 131 monthly CSV files (Feb 2013 â†’ Dec 2023)
- Format: UTF-16 LE, tab-separated, European decimals
- Pattern: `YYYY MM [Spanish Month] Todos los Inversores.csv`

### Conversion Script

```bash
# Install dependencies
pip install pandas pyarrow

# Convert one file
python src/data_conversion.py

### Output Structure
data/staging/monthly/
â””â”€â”€ year=2013/
â””â”€â”€ month=02/
â””â”€â”€ solar_data_2013_02.parquet

### Data Documentation
See [docs/data_dictionary.md](docs/data_dictionary.md) for column descriptions.

## Architecture

### Pipeline Overview
CSV (raw) â†’ Parquet â†’ Validation â†’ S3 Upload â†’ S3 Verification

### Local Development

The pipeline runs on Apache Airflow, containerized with Docker:

| Service | Purpose |
|---------|---------|
| PostgreSQL | Metadata database |
| Airflow Webserver | UI (localhost:8080) |
| Airflow Scheduler | Task orchestration |

### Setup

```bash
cd airflow
docker compose up airflow-init   # First time only
docker compose up -d             # Start services
Access the UI at http://localhost:8080 (admin/admin)

Pipeline DAG: solar_pipeline
Task	Description
convert_csv_to_parquet	Convert 131 monthly CSVs to Parquet format
verify_conversion	Validate file count, columns, format, year range
upload_to_s3	Upload to s3://solar-analytics-raw-scl-dev/
verify_s3_upload	Validate upload count, bucket, region
Tasks communicate via XCom. If any validation fails, downstream tasks are blocked.

## Project Structure
solar-analytics/
â”œâ”€â”€ terraform/
â”‚ â”œâ”€â”€ provider.tf
â”‚ â”œâ”€â”€ variables.tf
â”‚ â”œâ”€â”€ main.tf
â”‚ â”œâ”€â”€ outputs.tf
â”‚ â””â”€â”€ iam.tf
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_conversion.py
â”‚ â”œâ”€â”€ verify_conversion.py
â”‚ â”œâ”€â”€ upload_to_s3.py
â”‚ â””â”€â”€ verify_s3_upload.py
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ docker-compose.yaml
â”‚ â””â”€â”€ dags/
â”‚ â””â”€â”€ solar_pipeline_dag.py
â”œâ”€â”€ docs/
â”‚ â””â”€â”€ data_dictionary.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore