# Solar Performance Analytics Platform

Cloud-based pipeline for ISO 61724-compliant solar monitoring analysis.

## Project Overview

Migration of a paid university research collaboration project ("Beca de colaboraciÃ³n" at UPM) â€” 10 years of solar performance data â€” from CSV to a modern cloud data stack.

## Author

Sergio â€” Electrical Engineer transitioning to Data Engineering
Target: Data Engineer in Zurich (Energy Sector)

## Data

| Type | Count | Date Range |
|------|-------|------------|
| Monthly CSVs | 131 | Feb 2013 â†’ Dec 2023 |
| Auxiliary CSVs | 2 | â€” |
| Auxiliary XLSX | 13 | â€” |

## Architecture

| Layer | Technology | Status |
|-------|------------|--------|
| Storage | AWS S3 | âœ… |
| Infrastructure | Terraform | âœ… |
| Orchestration | Airflow (Docker) | âœ… |
| Validation | Custom (physical bounds) | âœ… |
| Transform | Spark + dbt | ğŸ”œ |
| Visualization | Streamlit | ğŸ”œ |

## S3 Structure

```
solar-analytics-raw-scl-dev/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ monthly/                  # 131 monthly CSV files
â”‚   â”‚   â””â”€â”€ year=YYYY/
â”‚   â”‚       â””â”€â”€ month=MM/
â”‚   â””â”€â”€ auxiliary/
â”‚       â”œâ”€â”€ csv/                  # 2 auxiliary CSVs
â”‚       â””â”€â”€ xlsx/                 # 13 auxiliary XLSX
â”œâ”€â”€ staging/
â”‚   â””â”€â”€ monthly/                  # Cleaned Parquet files
â””â”€â”€ curated/
    â””â”€â”€ performance_ratio/        # Final analytics
```

## Infrastructure

### Resources Created

| Resource | Name |
|----------|------|
| S3 Bucket | `solar-analytics-raw-scl-dev` |
| IAM Role | `solar-analytics-pipeline-role-dev` |
| IAM Policy | `solar-analytics-s3-access-dev` |

### Security

- âœ… S3 versioning enabled
- âœ… S3 public access blocked
- âœ… IAM role with least-privilege S3 access

### Deploy

```bash
cd terraform
terraform init
terraform apply
```

## Data Pipeline

### Source Data
- 131 monthly CSV files (Feb 2013 â†’ Dec 2023)
- Format: UTF-16 LE, tab-separated, European decimals
- Pattern: `YYYY MM [Spanish Month] Todos los Inversores.csv`

### Conversion Script

```bash
# Install dependencies
pip install pandas pyarrow

# Convert one file
python src/data_conversion.py
```

### Output Structure
```
data/staging/monthly/
â””â”€â”€ year=2013/
    â””â”€â”€ month=02/
        â””â”€â”€ solar_data_2013_02.parquet
```

### Data Documentation
See [docs/data_dictionary.md](docs/data_dictionary.md) for column descriptions.

## Pipeline

### Overview
```
CSV (raw) â†’ Parquet â†’ Validation â†’ Quality Check â†’ S3 Upload â†’ S3 Verification
```

### Local Development

The pipeline runs on Apache Airflow, containerized with Docker:

| Service | Purpose |
|---------|---------|
| PostgreSQL | Metadata database |
| Airflow Webserver | UI (localhost:8080) |
| Airflow Scheduler | Task orchestration |

### Setup

```bash
cd airflow
docker compose up airflow-init   # First time only
docker compose up -d             # Start services
```

Access the UI at http://localhost:8080 (admin/admin)

### Pipeline DAG: `solar_pipeline`

| Task | Description |
|------|-------------|
| `convert_csv_to_parquet` | Convert 131 monthly CSVs to Parquet format |
| `verify_conversion` | Validate file count, columns, format, year range |
| `validate_quality` | Physical bounds checking against ISO/AEMET limits |
| `upload_to_s3` | Upload to s3://solar-analytics-raw-scl-dev/ |
| `verify_s3_upload` | Validate upload count, bucket, region |

Tasks communicate via XCom. If any validation fails, downstream tasks are blocked â€” bad data never reaches S3.

## Data Quality

### Validation Approach

The `validate_quality` task performs 5 checks on every Parquet file:

| Check | Type | Action on Failure |
|-------|------|-------------------|
| File count (131) | Critical | âŒ Stop pipeline |
| Schema consistency (108 columns) | Critical | âŒ Stop pipeline |
| No empty files | Critical | âŒ Stop pipeline |
| Timestamp integrity | Warning | âš ï¸ Log, continue |
| Physical bounds | Warning | âš ï¸ Log, continue |

### Physical Bounds

Envelope bounds across all 13 solar systems, derived from ISO standards and AEMET historical data for Madrid:

| Variable | Min | Max | Unit |
|----------|-----|-----|------|
| Irradiance (G_H, G_M) | 0 | 1,500 | W/mÂ² |
| Ambient Temperature (T_U) | -17.4 | 50 | Â°C |
| Module Temperature (T_M) | -17.4 | 70 | Â°C |
| Inverter Temperature (T_WR) | -17.4 | 80 | Â°C |
| AC Power (P_AC) | 0 | 5,880 | W |
| DC Voltage (U_DC) | 0 | 548.5 | V |
| AC Voltage (U_AC) | 0 | 280 | V |

Per-system validation with auxiliary reference files planned for the dbt layer.

### Findings (1.1M rows, 131 files)

| Finding | Severity | Detail |
|---------|----------|--------|
| T_M3 sensor failure | High | Thousands of sub-minimum readings across most of the 10-year period |
| T_M2 sensor failure | High | Failed ~Aug 2021, stuck above 70Â°C through Dec 2022 |
| T_WR(11) runs hot | Low | Consistently highest inverter temperature every summer |
| T_WR(8), T_WR(3), T_WR(10) | Low | Seasonal summer exceedances, expected behavior |
| All T_M below -17.4Â°C | Medium | System-wide events in Apr 2019, Jan 2022 (communication loss) |
| U_DC(12) above 548.5V | Low | 6 total readings across 10 years |

These findings match the known sensor issues from the original university research. Temperature correction from ambient (T_U) is planned for the dbt transformation layer.

## Project Structure

```
solar-analytics/
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ iam.tf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_conversion.py
â”‚   â”œâ”€â”€ verify_conversion.py
â”‚   â”œâ”€â”€ upload_to_s3.py
â”‚   â”œâ”€â”€ verify_s3_upload.py
â”‚   â””â”€â”€ validate_parquet.py
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ solar_pipeline_dag.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_dictionary.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```